{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM Trigram Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U4nna79JcCg5Wlx5BGnfLXh47uYTO6y2",
      "authorship_tag": "ABX9TyOEfntuWYIUG1VqYPGaQr1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoy1729/Data_mining/blob/main/DM_Trigram_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN6Dk03Pkwu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cf0626-32b4-474f-f93c-9508a51051e9"
      },
      "source": [
        "!pip3 install python-docx\n",
        "!pip install git+https://github.com/banglakit/bengali-stemmer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.6/dist-packages (0.8.10)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Collecting git+https://github.com/banglakit/bengali-stemmer.git\n",
            "  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-85asdecq\n",
            "  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-85asdecq\n",
            "Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: bengali-stemmer\n",
            "  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=44604bbb5b896aaaddcf9b31e4f7b65c5c06c90e592476c2defad067ff5bff69\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f5ljwth6/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n",
            "Successfully built bengali-stemmer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgEs4JV-DCqo"
      },
      "source": [
        "import docx\n",
        "from bengali_stemmer.rafikamal2014 import RafiStemmer\n",
        "import string\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import csv\n",
        "from gensim.models import Phrases\n",
        "\n",
        "from gensim.models.phrases import Phraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUMrMN3ukkVF"
      },
      "source": [
        "\n",
        "\n",
        "def read_doc_as_pandasDF(filename):\n",
        "\n",
        "    data = pd.read_csv(filename, error_bad_lines=False)\n",
        "    data_text = data[['content']]\n",
        "\n",
        "    data_text['index'] = data_text.index\n",
        "    documents = data_text\n",
        "\n",
        "    return(documents)\n",
        "\n",
        "def punctuation_remover(text):\n",
        "    BENGALI_PUNCTUATION = string.punctuation + \"—।’‘\"\n",
        "    BENGALI_NUMERALS = \"০১২৩৪৫৬৭৮৯\"\n",
        "    return text.translate(str.maketrans(' ', ' ', BENGALI_PUNCTUATION+BENGALI_NUMERALS))\n",
        "\n",
        "def load_stop_word(doc_dir = r\"/content/drive/My Drive/Dm_Bigram/stopword-dictionary.docx\"):\n",
        "\n",
        "    stop_directory = doc_dir\n",
        "\n",
        "    doc = docx.Document(stop_directory)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "\n",
        "    bengali_stop_words = fullText[0].split()\n",
        "    bengali_stop_words = frozenset(bengali_stop_words)\n",
        "\n",
        "    return bengali_stop_words\n",
        "def preprocess_documents(doc):\n",
        "\n",
        "    preprocessed_list_of_docs = []\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "    preprocessed_docs = []\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(doc, str):\n",
        "        for token in punctuation_remover(doc).split():\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                if len(stemmer.stem_word(token)) >= 2:\n",
        "                    doc_token.append(stemmer.stem_word(token))\n",
        "\n",
        "\n",
        "    return doc_token\n",
        "def prepare_bag_of_words(processed_docs, dictionary):\n",
        "    return [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "def prepare_bow_list(bow_corpus, dictionary):\n",
        "\n",
        "    header_list = list(range(0, len(dictionary)-1))\n",
        "    all_list = [header_list]\n",
        "\n",
        "    for each_list in bow_corpus:\n",
        "        temp_list = [0]*len(dictionary)\n",
        "        for each_tuple in each_list:\n",
        "            temp_list[each_tuple[0]] = each_tuple[1]\n",
        "        all_list.append(temp_list)\n",
        "\n",
        "    minimal_all_list = []\n",
        "\n",
        "    minimal_header_list = []\n",
        "\n",
        "    for i in range(len(dictionary)):\n",
        "        minimal_header_list.append(dictionary[i])\n",
        "\n",
        "    minimal_all_list.append(minimal_header_list)\n",
        "\n",
        "    for each_mini_list in all_list[1:]:\n",
        "        minimal_all_list.append(each_mini_list)\n",
        "\n",
        "\n",
        "    return(minimal_all_list)  \n",
        "def get_sentence_list(pd_document):\n",
        "    sentence_list = []\n",
        "    for each_article in smaller_documents['content']:\n",
        "            #print(\"Running\")\n",
        "            if(isinstance(each_article, str)):\n",
        "                for each_line in each_article.split(\"।\"):\n",
        "                    for each_line_2 in each_line.split(\"?\"):\n",
        "                        sentence_list.append(each_line_2)\n",
        "\n",
        "\n",
        "    full_sentence_list = []\n",
        "    for each_sentence in sentence_list:\n",
        "        if (not each_sentence == \"\"):\n",
        "            full_sentence_list.append(each_sentence)\n",
        "\n",
        "    return(full_sentence_list)\n",
        "def write_sentence_to_csv(sentence_list, SENTENCE_LIST_CSV_DIR):\n",
        "\n",
        "    sentence_df = pd.DataFrame(sentence_list, columns=['content'])\n",
        "    sentence_df.to_csv(SENTENCE_LIST_CSV_DIR, sep=',',index=False)\n",
        "\n",
        "    return\n",
        "def preprocess_bigram_sentence(docs):\n",
        "\n",
        "    sentence_stream = []\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "\n",
        "    for doc in docs:\n",
        "\n",
        "        doc_token = []\n",
        "\n",
        "        if isinstance(doc, str):\n",
        "            for token in punctuation_remover(doc).split():\n",
        "                if token not in stop_words and len(token) >= 3:\n",
        "                    doc_token.append(token)\n",
        "\n",
        "        sentence_stream.append(doc_token)\n",
        "\n",
        "    return sentence_stream\n",
        "def get_bigram_list(full_sentence_list, stem = False):\n",
        "    sentence_stream = [doc.split(\" \") for doc in full_sentence_list]\n",
        "    #print(sentence_stream)\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "\n",
        "\n",
        "    bigram = Phrases(sentence_stream, min_count=2, threshold=5, delimiter=b'_')\n",
        "\n",
        "    bigram_phraser = Phraser(bigram)\n",
        "\n",
        "    bigram_list = []\n",
        "\n",
        "    #print(bigram_phraser)\n",
        "    for sent in sentence_stream:\n",
        "        tokens_ = bigram_phraser[sent]\n",
        "\n",
        "        for each_bigram in tokens_:\n",
        "            if each_bigram.count('_') == 1:\n",
        "                #print(each_bigram)\n",
        "                if stem == True:\n",
        "                    bigram_list.append(stemmer.stem_word(each_bigram))\n",
        "                else:\n",
        "                    bigram_list.append(each_bigram)\n",
        "\n",
        "    bigram_count_list = []\n",
        "    for each_unique_bigram in set(bigram_list):\n",
        "        bigram_count_list.append([each_unique_bigram, bigram_list.count(each_unique_bigram)])\n",
        "\n",
        "\n",
        "\n",
        "    return(bigram_count_list)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Sfo5OwC0uV",
        "outputId": "7aea043d-1376-4b2d-e000-6a638f454021"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    CSV_LOCATION = r\"/content/drive/My Drive/Dm_Bigram/amadershomoy.csv\"\n",
        "    SENTENCE_CSV_LOCATION = \"\"\n",
        "    \n",
        "    pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "    smaller_documents = pd_document[0:]\n",
        "    #processed_docs = smaller_documents['content'].map(preprocess_documents)\n",
        "\n",
        "    full_sentence_list = get_sentence_list(smaller_documents)\n",
        "\n",
        "    #Remove Punctuation from Each Sentence\n",
        "    punctuation_removed_full_sentence_list = [punctuation_remover(every_sentence) for every_sentence in full_sentence_list]\n",
        "\n",
        "    #Remove Stop word\n",
        "    stop_removed_sentence_stream = preprocess_bigram_sentence(punctuation_removed_full_sentence_list)\n",
        "\n",
        "    #print(stop_removed_sentence_stream[:10])\n",
        "\n",
        "\n",
        "    minimal_sentence_stream = stop_removed_sentence_stream\n",
        "    bigram = Phrases(minimal_sentence_stream, min_count=1, threshold=1, delimiter=b'_')\n",
        "    trigram = Phrases(bigram[minimal_sentence_stream], min_count=1, threshold = 1, delimiter=b'_')\n",
        "\n",
        "\n",
        "    bigram_phraser = Phraser(bigram)\n",
        "    trigram_phraser =  Phraser(trigram)\n",
        "\n",
        "\n",
        "    bigram_list = []\n",
        "    trigram_list = []\n",
        "\n",
        "\n",
        "    for sent in minimal_sentence_stream:\n",
        "        trigram_tokens_ =  trigram_phraser[bigram[sent]]\n",
        "\n",
        "        \n",
        "        for each_trigram in trigram_tokens_: \n",
        "            if each_trigram.count('_') == 2:\n",
        "                trigram_list.append(each_trigram)\n",
        "        \n",
        "\n",
        "\n",
        "    stemmed_trigram_count_list = []\n",
        "\n",
        "    stemmer = RafiStemmer()\n",
        "    stemmed_trigram_list = [stemmer.stem_word(each_non_stemmed_trigram) for each_non_stemmed_trigram in trigram_list]\n",
        "    for each_unique_stemmed_trigram in set(stemmed_trigram_list):\n",
        "        stemmed_trigram_count_list.append([each_unique_stemmed_trigram, stemmed_trigram_list.count(each_unique_stemmed_trigram)])\n",
        "\n",
        "\n",
        "\n",
        "    trigram_count_list = []\n",
        "\n",
        "    for each_unique_trigram in set(trigram_list):\n",
        "        trigram_count_list.append([each_unique_trigram, bigram_list.count(each_unique_trigram)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGa_lpM6C73c"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Dm_Bigram/non_stemmed_tri_gram_list_with_count.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(trigram_count_list)\n",
        "\n",
        "with open(\"/content/drive/My Drive/Dm_Bigram/stemmed_tri_gram_list_with_count.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(stemmed_trigram_count_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_84HF0QUG-G"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Dm_Bigram/sorted_stemmed_tri_gram.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        stemmed_trigram_count_list = sorted(stemmed_trigram_count_list)\n",
        "        writer.writerows(stemmed_trigram_count_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNnzMXAdbi5C"
      },
      "source": [
        "full_sentence_list\n",
        "with open(\"/content/drive/My Drive/Dm_Bigram/full_sentence_list.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(full_sentence_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}