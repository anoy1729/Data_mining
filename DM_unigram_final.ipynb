{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM unigram final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZTMkhaILwWam9VE9ycdzPE1OpD2sAl_M",
      "authorship_tag": "ABX9TyMPf1lh5yluzTbcyoV8Y7XM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoy1729/Data_mining/blob/main/DM_unigram_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC_d8_0AD8XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a4aa5c-c3cd-4c4d-bcf4-0f3148d166a5"
      },
      "source": [
        "!pip3 install python-docx\n",
        "!pip install git+https://github.com/banglakit/bengali-stemmer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=323db9d6d736ff5321da3a46da90ce8e0dff08439fa88ea0e5b15b3c705a5cb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n",
            "Collecting git+https://github.com/banglakit/bengali-stemmer.git\n",
            "  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-dfspy6w1\n",
            "  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-dfspy6w1\n",
            "Building wheels for collected packages: bengali-stemmer\n",
            "  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=c6b5711e18ce7048acccdf73d3927caed22a95e06b27e4beb9fe3708eb1566e6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5xgfdm77/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n",
            "Successfully built bengali-stemmer\n",
            "Installing collected packages: bengali-stemmer\n",
            "Successfully installed bengali-stemmer-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxbrLHwgD0RK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb297ab-5895-4dfe-805f-88335de53834"
      },
      "source": [
        "import docx\n",
        "from bengali_stemmer.rafikamal2014 import RafiStemmer\n",
        "import string\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import csv\n",
        "\n",
        "def read_doc_as_pandasDF(filename):\n",
        "\n",
        "    data = pd.read_csv(filename, error_bad_lines=False)\n",
        "    data_text = data[['content']]\n",
        "\n",
        "    data_text['index'] = data_text.index\n",
        "    documents = data_text\n",
        "\n",
        "    return(documents)\n",
        "def punctuation_remover(text):\n",
        "    BENGALI_PUNCTUATION = string.punctuation + \"—।’‘\"\n",
        "    BENGALI_NUMERALS = \"০১২৩৪৫৬৭৮৯\"\n",
        "    return text.translate(str.maketrans(' ', ' ', BENGALI_PUNCTUATION+BENGALI_NUMERALS))\n",
        "def load_stop_word(doc_dir = r\"/content/drive/My Drive/Dm_Bigram/stopword-dictionary.docx\"):\n",
        "\n",
        "    stop_directory = doc_dir\n",
        "\n",
        "    doc = docx.Document(stop_directory)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "\n",
        "    bengali_stop_words = fullText[0].split()\n",
        "    bengali_stop_words = frozenset(bengali_stop_words)\n",
        "\n",
        "    return bengali_stop_words\n",
        "def preprocess_documents(doc):\n",
        "\n",
        "    preprocessed_list_of_docs = []\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "    preprocessed_docs = []\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(doc, str):\n",
        "        for token in punctuation_remover(doc).split():\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                if len(stemmer.stem_word(token)) >= 2:\n",
        "                    doc_token.append(stemmer.stem_word(token))\n",
        "\n",
        "\n",
        "    return doc_token\n",
        "def prepare_bag_of_words(processed_docs, dictionary):\n",
        "    return [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "def prepare_bow_list(bow_corpus, dictionary):\n",
        "\n",
        "    header_list = list(range(0, len(dictionary)-1))\n",
        "    all_list = [header_list]\n",
        "\n",
        "    for each_list in bow_corpus:\n",
        "        temp_list = [0]*len(dictionary)\n",
        "        for each_tuple in each_list:\n",
        "            temp_list[each_tuple[0]] = each_tuple[1]\n",
        "        all_list.append(temp_list)\n",
        "\n",
        "    minimal_all_list = []\n",
        "\n",
        "    minimal_header_list = []\n",
        "\n",
        "    for i in range(len(dictionary)):\n",
        "        minimal_header_list.append(dictionary[i])\n",
        "\n",
        "    minimal_all_list.append(minimal_header_list)\n",
        "\n",
        "    for each_mini_list in all_list[1:]:\n",
        "        minimal_all_list.append(each_mini_list)\n",
        "\n",
        "\n",
        "    return(minimal_all_list)   \n",
        "def process_nonstemmed_documents(doc):\n",
        "    preprocessed_list_of_docs = []\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "    preprocessed_docs = []\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(doc, str):\n",
        "        for token in punctuation_remover(doc).split():\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                doc_token.append((token))\n",
        "\n",
        "\n",
        "    return doc_token\n",
        "\n",
        "def get_bow_list(bow_corpus, dictionary):\n",
        "\n",
        "    full_list  = [['token', 'count']]\n",
        "    token_list = [0]*len(dictionary)\n",
        "\n",
        "\n",
        "    for each_doc in bow_corpus:\n",
        "        for each_tuple in each_doc:\n",
        "            token_list[each_tuple[0]] += each_tuple[1]\n",
        "\n",
        "    for i, token_count in enumerate(token_list):\n",
        "        full_list.append([dictionary[i], token_count])\n",
        "\n",
        "    return(full_list)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_LOCATION = r\"/content/drive/My Drive/Dm_Bigram/amadershomoy.csv\"\n",
        "    \n",
        "    pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "    smaller_documents = pd_document[0:]\n",
        "    #processed_docs = smaller_documents['content'].map(preprocess_documents)\n",
        "    processed_docs = smaller_documents['content'].map(process_nonstemmed_documents)\n",
        "\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "    #dictionary.filter_extremes(no_below=.01, no_above=0.6, keep_n=100000)\n",
        "\n",
        "    bow_corpus = prepare_bag_of_words(processed_docs, dictionary)\n",
        "\n",
        "    #print(bow_corpus)\n",
        "\n",
        "    minimal_all_list = get_bow_list(bow_corpus, dictionary)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsoGgtqdLeeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0720a170-767d-41db-f051-7fefd6c0c80c"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Dm_Bigram/sorted_uni.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        minimal_all_list = sorted(minimal_all_list)\n",
        "        print(minimal_all_list)\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(minimal_all_list)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}