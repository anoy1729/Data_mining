{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM Bigram Final.ipynb",
      "provenance": [],
      "mount_file_id": "1n28PCCHg4v8BfW1cYIz6ekwujmkasXYp",
      "authorship_tag": "ABX9TyO1KoWwiuXGot4+EfE2ekT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoy1729/Data_mining/blob/main/DM_Bigram_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeKsFgsqwNNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9fe4c1-a642-4114-a8f1-52a7af73e18e"
      },
      "source": [
        "!pip3 install python-docx\n",
        "!pip install git+https://github.com/banglakit/bengali-stemmer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=702feb65b333e7e775232dd34b2da59c221b1f943974cb1fe857e0d04da2ef28\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n",
            "Collecting git+https://github.com/banglakit/bengali-stemmer.git\n",
            "  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-r84h6i55\n",
            "  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-r84h6i55\n",
            "Building wheels for collected packages: bengali-stemmer\n",
            "  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=061cd41f215a9ed5f4db7ea786e9c2f71c1550c3fdf6fc07dc4f4014b0b460d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vhvh3yxc/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n",
            "Successfully built bengali-stemmer\n",
            "Installing collected packages: bengali-stemmer\n",
            "Successfully installed bengali-stemmer-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx6Sfew2uSCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cff1d3b-84d4-4660-8fc4-3d100958fd8b"
      },
      "source": [
        "\n",
        "import docx\n",
        "from bengali_stemmer.rafikamal2014 import RafiStemmer\n",
        "import string\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import csv\n",
        "from gensim.models import Phrases\n",
        "\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "def read_doc_as_pandasDF(filename):\n",
        "\n",
        "    data = pd.read_csv(filename, error_bad_lines=False)\n",
        "    data_text = data[['content']]\n",
        "\n",
        "    data_text['index'] = data_text.index\n",
        "    documents = data_text\n",
        "\n",
        "    return(documents)\n",
        "\n",
        "def punctuation_remover(text):\n",
        "    BENGALI_PUNCTUATION = string.punctuation + \"—।’‘\"\n",
        "    BENGALI_NUMERALS = \"০১২৩৪৫৬৭৮৯\"\n",
        "    return text.translate(str.maketrans(' ', ' ', BENGALI_PUNCTUATION+BENGALI_NUMERALS))\n",
        "\n",
        "def load_stop_word(doc_dir = r\"/content/drive/My Drive/Dm_Bigram/stopword-dictionary.docx\"):\n",
        "\n",
        "    stop_directory = doc_dir\n",
        "\n",
        "    doc = docx.Document(stop_directory)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "\n",
        "    bengali_stop_words = fullText[0].split()\n",
        "    bengali_stop_words = frozenset(bengali_stop_words)\n",
        "\n",
        "    return bengali_stop_words\n",
        "def preprocess_documents(doc):\n",
        "\n",
        "    preprocessed_list_of_docs = []\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "    preprocessed_docs = []\n",
        "\n",
        "    doc_token = []\n",
        "\n",
        "    if isinstance(doc, str):\n",
        "        for token in punctuation_remover(doc).split():\n",
        "            if token not in stop_words and len(token) >= 3:\n",
        "                if len(stemmer.stem_word(token)) >= 2:\n",
        "                    doc_token.append(stemmer.stem_word(token))\n",
        "\n",
        "\n",
        "    return doc_token\n",
        "def prepare_bag_of_words(processed_docs, dictionary):\n",
        "    return [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "def prepare_bow_list(bow_corpus, dictionary):\n",
        "\n",
        "    header_list = list(range(0, len(dictionary)-1))\n",
        "    all_list = [header_list]\n",
        "\n",
        "    for each_list in bow_corpus:\n",
        "        temp_list = [0]*len(dictionary)\n",
        "        for each_tuple in each_list:\n",
        "            temp_list[each_tuple[0]] = each_tuple[1]\n",
        "        all_list.append(temp_list)\n",
        "\n",
        "    minimal_all_list = []\n",
        "\n",
        "    minimal_header_list = []\n",
        "\n",
        "    for i in range(len(dictionary)):\n",
        "        minimal_header_list.append(dictionary[i])\n",
        "\n",
        "    minimal_all_list.append(minimal_header_list)\n",
        "\n",
        "    for each_mini_list in all_list[1:]:\n",
        "        minimal_all_list.append(each_mini_list)\n",
        "\n",
        "\n",
        "    return(minimal_all_list)  \n",
        "def get_sentence_list(pd_document):\n",
        "    sentence_list = []\n",
        "    for each_article in smaller_documents['content']:\n",
        "            #print(\"Running\")\n",
        "            if(isinstance(each_article, str)):\n",
        "                for each_line in each_article.split(\"।\"):\n",
        "                    for each_line_2 in each_line.split(\"?\"):\n",
        "                        sentence_list.append(each_line_2)\n",
        "\n",
        "\n",
        "    full_sentence_list = []\n",
        "    for each_sentence in sentence_list:\n",
        "        if (not each_sentence == \"\"):\n",
        "            full_sentence_list.append(each_sentence)\n",
        "\n",
        "    return(full_sentence_list)\n",
        "def write_sentence_to_csv(sentence_list, SENTENCE_LIST_CSV_DIR):\n",
        "\n",
        "    sentence_df = pd.DataFrame(sentence_list, columns=['content'])\n",
        "    sentence_df.to_csv(SENTENCE_LIST_CSV_DIR, sep=',',index=False)\n",
        "\n",
        "    return\n",
        "def preprocess_bigram_sentence(docs):\n",
        "\n",
        "    sentence_stream = []\n",
        "\n",
        "    stop_words = load_stop_word()\n",
        "\n",
        "    for doc in docs:\n",
        "\n",
        "        doc_token = []\n",
        "\n",
        "        if isinstance(doc, str):\n",
        "            for token in punctuation_remover(doc).split():\n",
        "                if token not in stop_words and len(token) >= 3:\n",
        "                    doc_token.append(token)\n",
        "\n",
        "        sentence_stream.append(doc_token)\n",
        "\n",
        "    return sentence_stream\n",
        "def get_bigram_list(full_sentence_list, stem = False):\n",
        "    sentence_stream = [doc.split(\" \") for doc in full_sentence_list]\n",
        "    #print(sentence_stream)\n",
        "    stemmer = RafiStemmer()\n",
        "\n",
        "\n",
        "\n",
        "    bigram = Phrases(sentence_stream, min_count=2, threshold=5, delimiter=b'_')\n",
        "\n",
        "    bigram_phraser = Phraser(bigram)\n",
        "\n",
        "    bigram_list = []\n",
        "\n",
        "    #print(bigram_phraser)\n",
        "    for sent in sentence_stream:\n",
        "        tokens_ = bigram_phraser[sent]\n",
        "\n",
        "        for each_bigram in tokens_:\n",
        "            if each_bigram.count('_') == 1:\n",
        "                #print(each_bigram)\n",
        "                if stem == True:\n",
        "                    bigram_list.append(stemmer.stem_word(each_bigram))\n",
        "                else:\n",
        "                    bigram_list.append(each_bigram)\n",
        "\n",
        "    bigram_count_list = []\n",
        "    for each_unique_bigram in set(bigram_list):\n",
        "        bigram_count_list.append([each_unique_bigram, bigram_list.count(each_unique_bigram)])\n",
        "\n",
        "\n",
        "\n",
        "    return(bigram_count_list)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_LOCATION = r\"/content/drive/My Drive/Dm_Bigram/amadershomoy.csv\"\n",
        "    SENTENCE_CSV_LOCATION = \"\"\n",
        "    \n",
        "    pd_document = read_doc_as_pandasDF(CSV_LOCATION)\n",
        "\n",
        "    smaller_documents = pd_document[0:]\n",
        "    #processed_docs = smaller_documents['content'].map(preprocess_documents)\n",
        "\n",
        "    full_sentence_list = get_sentence_list(smaller_documents)\n",
        "\n",
        "    #Remove Punctuation from Each Sentence\n",
        "    punctuation_removed_full_sentence_list = [punctuation_remover(every_sentence) for every_sentence in full_sentence_list]\n",
        "\n",
        "    #Remove Stop word\n",
        "    stop_removed_sentence_stream = preprocess_bigram_sentence(punctuation_removed_full_sentence_list)\n",
        "\n",
        "    #print(stop_removed_sentence_stream[:10])\n",
        "\n",
        "\n",
        "    minimal_sentence_stream = stop_removed_sentence_stream\n",
        "    bigram = Phrases(minimal_sentence_stream, min_count=1, threshold=1, delimiter=b'_')\n",
        "\n",
        "\n",
        "    bigram_phraser = Phraser(bigram)\n",
        "\n",
        "\n",
        "    bigram_list = []\n",
        "\n",
        "    for sent in minimal_sentence_stream:\n",
        "        tokens_ = bigram_phraser[sent]\n",
        "\n",
        "        \n",
        "        for each_bigram in tokens_:\n",
        "            if '_' in each_bigram:\n",
        "                #print(each_bigram)\n",
        "                bigram_list.append(each_bigram)\n",
        "        \n",
        "\n",
        "        # for each_trigram in trigram_tokens_:\n",
        "        #     #print(each_trigram)\n",
        "        #     if each_trigram.count('_') == 2:\n",
        "        #         trigram_list.append(each_trigram)\n",
        "\n",
        "    #print(len(bigram_list))\n",
        "\n",
        "\n",
        "    stemmed_bigram_count_list = []\n",
        "\n",
        "    stemmer = RafiStemmer()\n",
        "    stemmed_bigram_list = [stemmer.stem_word(each_non_stemmed_bigram) for each_non_stemmed_bigram in bigram_list]\n",
        "    for each_unique_stemmed_bigram in set(stemmed_bigram_list):\n",
        "        stemmed_bigram_count_list.append([each_unique_stemmed_bigram, stemmed_bigram_list.count(each_unique_stemmed_bigram)])\n",
        "\n",
        "\n",
        "\n",
        "    bigram_count_list = []\n",
        "\n",
        "    for each_unique_bigram in set(bigram_list):\n",
        "        bigram_count_list.append([each_unique_bigram, bigram_list.count(each_unique_bigram)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Dm_Bigram/non_stemmed_bi_gram_list_with_count.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(bigram_count_list)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Dm_Bigram/stemmed_bi_gram_list_with_count.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(stemmed_bigram_count_list)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UIWynFjtxL"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Dm_Bigram/sorted_bigram.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        bigram_count_list = sorted(bigram_count_list)\n",
        "        writer.writerows(bigram_count_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}